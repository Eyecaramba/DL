__*INTRO*__  
AI model은 여러 block들로 이루어져 있다.  
이 block을 구현하는 여러 class를 모아놓은 package가 바로 torch.nn이다.    

<br>

__*목차*__  
1. Container
2. Parameter
3. Layer
4. nn.Module 분석하기     
5. Pytorch hook  
6. Pytorch apply

<br>

## 1. [Container](https://pytorch.org/docs/stable/nn.html#containers)
___
여러 block을 하나로 묶는 Container 역할을 하는 Class들이 있다.  
가장 기본이 되는 Container Module은 torch.nn.Module이다.
```python
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
```
위와 같이 하나의 모델을 만들 때 nn.Module을 상속해서 모델을 만들게 된다.  
모델에는 forward함수가 반드시 선언되어 있어야 한다. 

기본 Moudle 이외에도 목적에 맞게 Sequential,ModuleList, MoudleDict을 활용할 수 있다.   

- Sequential : 묶어놓은 Module을 차례차례 사용하고 싶은 경우
 - ModuleList : list처럼 묶어놓은 Module을 index를 통해 사용하고 싶은 경우  
 - ModuleDict : Dict처럼 묶언놓은 Module을 key를 통해 사용하고 싶은 경우 

python에서 제공되는 기본적인 자료구조를 활용하지 않고 위와 같은 module을 활용하는 이유는 해당 module이 위 Module에 등록되지 않기 때문이다.  


parameter만 묶는 ParameterList, ParameterDict도 있다.  


## 2. [Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter), [buffer](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer)
___
Module안에서 학습의 대상이 되는 Class. 
tensor의 subclass로 Module의 속성으로 지정되면 자동으로 Module의 state_dict에 등록된다.  
(반면 그냥 tensor로 등록한 경우 Module의 parameter list에 저장되지 않는다.)

buffer의 경우는 Module의 state_dict에 등록은 되지만 Module안에서 학습의 대상이 되지는 않는다.

## 3. Layer
---
...

## 4. nn.Module 분석하기
__Pytorch Module Method를 활용한 방법__ 

Module의 method를 통해 어떤 Moudle, Parameter, buffer로 이루어져 있는지 확인할 수 있다. 

- __Module 확인 method__  
`named_children` : 한 단계 아래에 있는 Module의 이름과 Moudle을 반환한다.   
`named_modules` : 해당 Module에 속한 모든 submodule을 전부 반환한다.   
`get_submodule` : 특정한 Module만을 불러올 때 사용한다.  

- __Parameter 확인 method__    
`named_parameters`: Module에 있는 모든 Parameter의 이름과 Paramter를 반환한다.  
`get_parameters` : Module에 있는 특정 Parameter를 불러온다.  

- __ Buffer 확인 method__       
`named_buffers` : Module 안에 있는 모든 buffer의 이름과 buffer를 반환받음   
`get_buffers` : Module의 특정 buffer를 반환받음  
<br>

__외부 라이브러리를 활용한 방법__

`torchsummary` 라이브러리 활용
```python
from torchsummary import summary
summary(model,input_size,batch_size=1,device='cuda')
```  
Module의 파라미터 개수, 용량 사이즈, 최대 용량, layer의 흐름 등등 전반적인 Module의 모습을 출력해준다.  

## 5. Pytorch Hook
___
Hook은 어떤 이벤트가 발생하기 전 또는 후에 작동하는 함수이다.  
Pytorch에서는 Module 또는 Tensor에 Hook을 설정할 수 있다.  

Module의 경우 Forward와 Backward 함수가 작동할 때 Hook을 등록할 수 있다.  
Module에는 다음의 method를 Hook을 등록할 수 있다.  

- __forward_pre_hooks__  
- __forward_hooks__  
- __backward_hooks__
- __full_backward_hooks__

Tensor는 다음의 method를 이용하여 Hook을 등록할 수 있다. 
- __register_hook__ 

<br>
<br>   

__Hook의 필요성__  

1. Hook을 활용하면 Model의 정보에 대해 깔끔하게 출력할 수 있다.  
2. Pre-trained network에서 여러 feature들을 불러올 수 있다.  
3. Hook을 통해 forward와 backward시에 weight의 값, gradient의 값을 상세하게 확인할 수 있다. 


즉 hook을 통해 사용자는 Model이 작동하는 과정에 상세하게 관여할 수 있다.  


## 6. Pytorch apply
---- 
Module안에 있는 모든 submodule에 어떤 함수를 적용하고자 할 때 사용된다.  
apply를 Module에 적용하면 함수를 submodule에 post order 순으로 적용된 Module이 반환된다. 
<br>
<br>
<br>

### Ref
----
https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904  