__*INTRO*__  
대부분의 딥러닝 모델들이 공통적으로 가지고 있는 학습 구조를 정리
<br>

__*목차*__
1. pytorch train 기본구조 code

<br>  

## 1. pytorch train 기본구조 code    
----   
딥러닝의 모델이 가지고 있는 기본 구조를 정리했습니다.  

1. __model__ 만들기
2. __dataset__, __dataloader__ 만들기 
3. __optimizer__ 와 __lossfunction__ 정의하기 
4. __학습__ 시키기
5. 학습 결과 __시각화__(시각화는 부수적으로 넣은 것입니다.)

위 기본 구조를 `code`로 확인해봅시다.  

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from dataset import ExampleDataset




############ 1단계 ############

# 1.모델을 만들거나(1-1) 기존에 있던 모델 불러오기(1-2)

# 1-1) custom model은 만든 경우 
class ExampleModel(nn.Module):
    def __init__(self):
        super().__init()
        self.layer1 = nn.Exlayer(..)
        self.layer2 = nn.Exlayer(..)
        ...
    
    def forward(self,x):
        x = F.relu(self.layer1(x))
        ...
        return F.relu(self.layer2(x))
model = ExampleModel()

# 1-2) pre-trained model을 불러오는 경우 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = models.PretrainedModel(pretrained = True).to(device)

# 1-3) model train mode로 바꾸기
model.train()

## 1-4) model 구조 확인하기 
from torchsummary import summary
summary(model,(...))

## 1-5) model 저장하고 불러오기  
MODEL_PATH = 'saved'
torch.save(model, os.path.join(MODEL_PATH, "model.pt"))
model = torch.load(os.path.join(MODEL_PATH, "model.pt"))





############ 2단계 ############

# 2-1) 데이터셋 만들기 
dataset_example = ExampleDataset()

# 2-2)데이터로더 만들기 
dataloader_example = DataLoader(dataset_example)




############ 3단계 ############

# 3-1) 옵티마이저 정의하기   
optimizer = optim.Example(model.parameteres(), lr=lr, )

# 3-2) 손실함수 정의하기   
loss_fn = nn.ExampleLoss()

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('logdir')

import wandb
config={"epochs": EPOCHS, "batch_size": BATCH_SIZE, "learning_rate" : LEARNING_RATE}
wandb.init(project = 'project_name',entity='entity_name',config = config)




############ 4단계 ############

# 4. 학습하기  
for e in range(epochs):
    for X,y in dataloader_example:
        output = model(X)
        epoch_loss = loss_fn(output, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


    # 학습결과 저장하기
    torch.save({
    'epoch': e,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': epoch_loss,
    }, f"training_result_epoch_{e}.pt")

    # writer로 tensorboard에 시각화할 정보 저장하기 
    writter.exfunctions(...)

    # Wandb로 시각화할 정보 저장하기 
    wandb.log({...})





############ 5단계 ############

# 5. 학습 결과 시각화
from torch.utils.tensorboard import SummaryWriter

# 5-1) tensorboard 결과 확인하기 
%tensorboard --logdir "app"

# 5-2) WandB : WandB site에서 결과 확인하기
```

